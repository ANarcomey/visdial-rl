{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "from six import iteritems\n",
    "import h5py\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "\n",
    "import options\n",
    "from utils import utilities as utils\n",
    "from dataloader import VisDialDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from eval_utils.rank_answerer import rankABot\n",
    "from eval_utils.rank_questioner import rankQBot\n",
    "from utils import utilities as utils\n",
    "from utils.visualize import VisdomVisualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \n",
    "    # A-Bot checkpoint\n",
    "    'startFrom': \"./checkpoints/color/abot_ep_15.vd\",\n",
    "    \n",
    "    # Q-Bot checkpoint should given if interactive dialog is required\n",
    "    # 'qstartFrom': \"./checkpoints/qbot_sl.vd\",\n",
    "    \n",
    "    'beamSize': 5,\n",
    "    'imgFeatureSize':16384,\n",
    "    'inputImg': 'data/visdial/data_1.0_img.h5',    \n",
    "    \n",
    "}\n",
    "\n",
    "static_params = {\n",
    "    'numRounds':10,\n",
    "    'useGPU': False,\n",
    "    'imgNorm': 0,\n",
    "    \n",
    "    'inputJson': \"/scr/anarc/motm/data/visdial/data/visdial_params_v2.json\",\n",
    "    'inputQues': '/scr/anarc/motm/data/visdial/data/visdial_data_v2.h5',\n",
    "    'cocoDir': '/scr/anarc/motm/data/visdial/data/visdial_images',\n",
    "    'cocoInfo': '/scr/anarc/motm/data/visdial/data/visdial_images/coco_info.json',\n",
    "}\n",
    "\n",
    "for key, value in iteritems(static_params):\n",
    "    params[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['test']\n",
    "dataset = VisDialDataset(params, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# RNG seed\n",
    "manualSeed = 1597\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "if params['useGPU']:\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "print('Loading json file: ' + params['inputJson'])\n",
    "with open(params['inputJson'], 'r') as fileId:\n",
    "    info = json.load(fileId)\n",
    "\n",
    "wordCount = len(info['word2ind'])\n",
    "# Add <START> and <END> to vocabulary\n",
    "info['word2ind']['<START>'] = wordCount + 1\n",
    "info['word2ind']['<END>'] = wordCount + 2\n",
    "startToken = info['word2ind']['<START>']\n",
    "endToken = info['word2ind']['<END>']\n",
    "# Padding token is at index 0\n",
    "vocabSize = wordCount + 3\n",
    "print('Vocab size with <START>, <END>: %d' % vocabSize)\n",
    "\n",
    "# Construct the reverse map\n",
    "info['ind2word'] = {\n",
    "    int(ind): word\n",
    "    for word, ind in info['word2ind'].items()\n",
    "}\n",
    "    \n",
    "def loadModel(params, agent='abot'):\n",
    "    # should be everything used in encoderParam, decoderParam below\n",
    "    encoderOptions = [\n",
    "        'encoder', 'vocabSize', 'embedSize', 'rnnHiddenSize', 'numLayers',\n",
    "        'useHistory', 'useIm', 'imgEmbedSize', 'imgFeatureSize', 'numRounds',\n",
    "        'dropout'\n",
    "    ]\n",
    "    decoderOptions = [\n",
    "        'decoder', 'vocabSize', 'embedSize', 'rnnHiddenSize', 'numLayers',\n",
    "        'dropout'\n",
    "    ]\n",
    "    modelOptions = encoderOptions + decoderOptions\n",
    "\n",
    "    mdict = None\n",
    "    gpuFlag = params['useGPU']\n",
    "    startArg = 'startFrom' if agent == 'abot' else 'qstartFrom'\n",
    "    assert params[startArg], \"Need checkpoint for {}\".format(agent)\n",
    "\n",
    "    if params[startArg]:\n",
    "        print('Loading model (weights and config) from {}'.format(\n",
    "            params[startArg]))\n",
    "\n",
    "        if gpuFlag:\n",
    "            mdict = torch.load(params[startArg])\n",
    "        else:\n",
    "            mdict = torch.load(params[startArg],\n",
    "                map_location=lambda storage, location: storage)\n",
    "\n",
    "        # Model options is a union of standard model options defined\n",
    "        # above and parameters loaded from checkpoint\n",
    "        modelOptions = list(set(modelOptions).union(set(mdict['params'])))\n",
    "        for opt in modelOptions:\n",
    "            if opt not in params:\n",
    "                params[opt] = mdict['params'][opt]\n",
    "\n",
    "            elif params[opt] != mdict['params'][opt]:\n",
    "                # Parameters are not overwritten from checkpoint\n",
    "                pass\n",
    "\n",
    "    # Initialize model class\n",
    "    encoderParam = {k: params[k] for k in encoderOptions}\n",
    "    decoderParam = {k: params[k] for k in decoderOptions}\n",
    "\n",
    "    encoderParam['startToken'] = encoderParam['vocabSize'] - 2\n",
    "    encoderParam['endToken'] = encoderParam['vocabSize'] - 1\n",
    "    decoderParam['startToken'] = decoderParam['vocabSize'] - 2\n",
    "    decoderParam['endToken'] = decoderParam['vocabSize'] - 1\n",
    "\n",
    "    if agent == 'abot':\n",
    "        encoderParam['type'] = params['encoder']\n",
    "        decoderParam['type'] = params['decoder']\n",
    "        encoderParam['isAnswerer'] = True\n",
    "        from visdial.models.answerer import Answerer\n",
    "        model = Answerer(encoderParam, decoderParam)\n",
    "        print(\"e param = \", encoderParam)\n",
    "        print(\"e = \", model.encoder)\n",
    "\n",
    "    elif agent == 'qbot':\n",
    "        encoderParam['type'] = params['qencoder']\n",
    "        decoderParam['type'] = params['qdecoder']\n",
    "        encoderParam['isAnswerer'] = False\n",
    "        encoderParam['useIm'] = False\n",
    "        from visdial.models.questioner import Questioner\n",
    "        model = Questioner(\n",
    "            encoderParam,\n",
    "            decoderParam,\n",
    "            imgFeatureSize=encoderParam['imgFeatureSize'])\n",
    "\n",
    "    if params['useGPU']:\n",
    "        model.cuda()\n",
    "\n",
    "    if mdict:\n",
    "        model.load_state_dict(mdict['model'])\n",
    "        \n",
    "    print(\"Loaded agent {}\".format(agent))\n",
    "    return model\n",
    "\n",
    "aBot = None\n",
    "qBot = None\n",
    "\n",
    "# load aBot\n",
    "if params['startFrom']:\n",
    "    aBot = loadModel(params, 'abot')\n",
    "    assert aBot.encoder.vocabSize == vocabSize, \"Vocab size mismatch!\"\n",
    "    aBot.eval()\n",
    "\n",
    "# load qBot\n",
    "if params['qstartFrom']:\n",
    "    qBot = loadModel(params, 'qbot')\n",
    "    assert qBot.encoder.vocabSize == vocabSize, \"Vocab size mismatch!\"\n",
    "    qBot.eval()\n",
    "\n",
    "# load pre-trained VGG 19\n",
    "print(\"Loading image feature extraction model\")\n",
    "feat_extract_model = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "feat_extract_model.classifier = nn.Sequential(*list(feat_extract_model.classifier.children())[:-3])\n",
    "# print(feat_extract_model)\n",
    "feat_extract_model.eval()\n",
    "\n",
    "if params['useGPU']:\n",
    "    feat_extract_model.cuda()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demo image, question and dialog history\n",
    "\n",
    "img_path = \"demo/img.jpg\"\n",
    "img_mat = skimage.io.imread(img_path)\n",
    "\n",
    "with open(\"demo/hist.json\") as hfile:\n",
    "    hist_info = json.load(hfile)\n",
    "\n",
    "with open(\"demo/ques.json\") as qfile:\n",
    "    ques_info = json.load(qfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_index = 4\n",
    "question_turn_index = -1\n",
    "\n",
    "example = dataset[example_index]\n",
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset.collate_fn([example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example['ques'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numRounds = dataset.numRounds\n",
    "\n",
    "sortedScoreAll = []\n",
    "logProbsAll = [[] for _ in range(numRounds)]\n",
    "\n",
    "scoringFunction=utils.maskedNll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_str_opt(opt):\n",
    "    print(opt)\n",
    "    opt_str = [info['ind2word'].get(ind,'UNK') for ind in opt.data]\n",
    "    opt_str = \n",
    "    print(opt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_str_opt(options[0][6][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankOptions(options, scores):\n",
    "    '''Rank a batch of examples against a list of options.'''\n",
    "    numOptions = options.size(1)\n",
    "    \n",
    "    # Sort all predicted scores\n",
    "    sortedScore, sortedInds = torch.sort(scores, 1)\n",
    "    #print(\"s = \", scores)\n",
    "    #print(\"ss = \", sortedScore)\n",
    "    #print(\"si = \", sortedInds)\n",
    "    \n",
    "    #sortedAnswers = [options_str[i] for i in sortedInds.data[0]]\n",
    "    #sortedAnswersStr = [raw_data['data']['answers'][i] for i in sortedAnswers]\n",
    "    #print(\"sa = \", sortedAnswersStr)\n",
    "    return sortedScore, sortedInds\n",
    "    \n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Variable(example['img_feat'], volatile=True)\n",
    "caption = Variable(example['cap'], volatile=True)\n",
    "captionLens = Variable(example['cap_len'], volatile=True)\n",
    "questions = Variable(example['ques'], volatile=True)\n",
    "quesLens = Variable(example['ques_len'], volatile=True)\n",
    "answers = Variable(example['ans'], volatile=True)\n",
    "ansLens = Variable(example['ans_len'], volatile=True)\n",
    "options = Variable(example['opt'], volatile=True)\n",
    "optionLens = Variable(example['opt_len'], volatile=True)\n",
    "#correctOptionInds = Variable(example['ans_id'], volatile=True)\n",
    "\n",
    "aBot.reset()\n",
    "aBot.observe(-1, image=image, caption=caption, captionLens=captionLens)\n",
    "for round in range(numRounds):\n",
    "    print(\"Round = \", round)\n",
    "    if quesLens[0][round].data[0] == 1:\n",
    "        print(\"skipping round\")\n",
    "        continue\n",
    "    if round==7:\n",
    "        print(\"q = \", questions[:,round])\n",
    "        print(\"ql = \", quesLens[:,round])\n",
    "        print(\"a = \", answers[:,round])\n",
    "        print(\"al = \", ansLens[:,round])\n",
    "    aBot.observe(\n",
    "        round,\n",
    "        ques=questions[:, round],\n",
    "        quesLens=quesLens[:, round],\n",
    "        ans=answers[:, round],\n",
    "        ansLens=ansLens[:, round])\n",
    "    print(\"opt = \", options[:,round])\n",
    "    logProbs = aBot.evalOptions(options[:, round],\n",
    "                                optionLens[:, round], scoringFunction)\n",
    "    #print(\"lp = \", logProbs)\n",
    "    logProbsCurrent = aBot.forward()\n",
    "    #print(\"lpc = \", logProbsCurrent)\n",
    "    logProbsAll[round].append(\n",
    "        scoringFunction(logProbsCurrent,\n",
    "                        answers[:, round].contiguous()))\n",
    "    #print(\"lpa = \", logProbsAll)\n",
    "    sortedScore, sortedInds = rankOptions(options[:,round], logProbs)\n",
    "    sortedScoreAll.append((sortedScore,sortedInds))\n",
    "    #batchRanks = rankOptions(options[:, round],\n",
    "    #                         correctOptionInds[:, round], logProbs)\n",
    "    #ranks.append(batchRanks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad image id to 12 digits with zeros out front, and add jpg extension\n",
    "def image_id_to_suffix(image_id):\n",
    "    return str(image_id).zfill(12) + '.jpg'\n",
    "\n",
    "def get_image_path(image_id, images_path):\n",
    "    for path in images_path:\n",
    "        filename = path + image_id_to_suffix(image_id)\n",
    "        if os.path.exists(filename):\n",
    "            #info_json['images'].append({'id':image_id,'file_path':\n",
    "            #                  os.path.join(os.path.basename(os.path.dirname(path)), os.path.basename(filename))})\n",
    "            #second to last dir in path is train2014 or val2014, join with image filename\n",
    "            return filename\n",
    "        \n",
    "    raise ValueError(\"Image id \\\"{}\\\" could not be found in given paths \\\"{}\\\"\"\n",
    "                     .format(image_id, images_path))\n",
    "\n",
    "\n",
    "def visualize_example(dialogue_entry, questions, answers, images_path, verbose=False):\n",
    "    image_id = dialogue_entry['image_id']\n",
    "    #image_filenames = [path + str(dialogue_entry['image_id']).zfill(12) + '.jpg' for path in images_path]\n",
    "    image_filenames = [path + image_id_to_suffix(dialogue_entry['image_id']) for path in images_path]\n",
    "    \n",
    "    if len(images_path) == 1:\n",
    "        image_filename = image_filenames[0]\n",
    "        display(Image(filename=image_filename))\n",
    "    elif len(images_path) == 2:\n",
    "        if os.path.exists(image_filenames[0]):\n",
    "            image_filename = image_filenames[0]\n",
    "            display(Image(filename=image_filename))\n",
    "        elif os.path.exists(image_filenames[1]):\n",
    "            image_filename = image_filenames[1]\n",
    "            display(Image(filename=image_filename))\n",
    "        else: \n",
    "            image_filename = None\n",
    "            print(\"Image could not be found.\")\n",
    "    else:\n",
    "        raise ValueError(\"Please update visualize_example() to search more than 2 possible image names.\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nDialogue entry: \\n{}\".format(dialogue_entry))\n",
    "        print(\"Image from filename {}\\n\".format(image_filename))\n",
    "        \n",
    "    print(\"Caption = \\\"{}\\\"\\n\".format(dialogue_entry['caption']))\n",
    "    for turn in dialogue_entry['dialog']:\n",
    "        question_id = turn['question']\n",
    "        print(\"Question = \\\"{}\\\"\".format(questions[question_id]))\n",
    "        if 'answer' in turn:\n",
    "            answer_id = turn['answer']\n",
    "            print(\"\\t\\t\\t\\tAnswer = \\\"{}\\\"\\n\".format(answers[answer_id]))\n",
    "        else:\n",
    "            answer_options_ids = turn['answer_options']\n",
    "            print(\"\\t\\t\\t\\tAnswer options = \\n{}\".format([answers[a_id] for a_id in answer_options_ids]))\n",
    "            \n",
    "inputs_test = {\n",
    "                            \"dialog_path\":\"visdial_1.0_test.json\",\n",
    "                            \"image_locations\":[\"visdial_images/VisualDialog_test2018\"],\n",
    "                            \"image_prefix\": [\"VisualDialog_test2018_\"]\n",
    "               }      \n",
    "\n",
    "data_basedir = \"../data/visdial/data\"\n",
    "\n",
    "dialog_path = os.path.join(data_basedir, inputs_test[\"dialog_path\"])\n",
    "image_paths = [os.path.join(data_basedir, location, prefix) \n",
    "                for location, prefix in list(zip(inputs_test[\"image_locations\"],inputs_test[\"image_prefix\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(dialogue_entry, questions, answers, images_path, sortedScoreAll, verbose=False):\n",
    "    \n",
    "    image_id = dialogue_entry['image_id']\n",
    "    #image_filenames = [path + str(dialogue_entry['image_id']).zfill(12) + '.jpg' for path in images_path]\n",
    "    image_filename = get_image_path(image_id, images_path)\n",
    "    display(Image(filename=image_filename))\n",
    "    \n",
    "        \n",
    "    print(\"Caption = \\\"{}\\\"\\n\".format(dialogue_entry['caption']))\n",
    "    for i, turn in enumerate(dialogue_entry['dialog']):\n",
    "        question_id = turn['question']\n",
    "        print(\"\\nQuestion = \\\"{}\\\"\".format(questions[question_id]))\n",
    "        if 'answer' in turn:\n",
    "            answer_id = turn['answer']\n",
    "            print(\"\\t\\t\\t\\tGround Truth Answer = \\\"{}\\\"\\n\".format(answers[answer_id]))\n",
    "        else:\n",
    "            answer_options_ids = turn['answer_options']\n",
    "            print(\"\\t\\t\\t\\tAnswer options = \\n{}\".format([answers[a_id] for a_id in answer_options_ids]))\n",
    "        #print(\"i=\",i)\n",
    "        #print(question_turn_index)\n",
    "        #print(len(dialogue_entry['dialog'])+i)\n",
    "        if i==question_turn_index or i==len(dialogue_entry['dialog'])+question_turn_index:\n",
    "            sortedScores, sortedInds = sortedScoreAll[i]\n",
    "            sortedAnswers = [options_str[i] for i in sortedInds.data[0]]\n",
    "            sortedAnswersStr = [answers[i] for i in sortedAnswers]\n",
    "            print(\"\\t\\t\\t\\tSelected answers = {}\".format(list(zip(sortedAnswersStr, list(sortedScores[0].data)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dialog_path = os.path.join(\"../data/visdial/data\", \"visdial_1.0_test.json\")\n",
    "inputs_test = {\n",
    "                            \"dialog_path\":\"visdial_1.0_test.json\",\n",
    "                            \"image_locations\":[\"visdial_images/VisualDialog_test2018\"],\n",
    "                            \"image_prefix\": [\"VisualDialog_test2018_\"]\n",
    "               }      \n",
    "\n",
    "data_basedir = \"../data/visdial/data\"\n",
    "\n",
    "dialog_path = os.path.join(data_basedir, inputs_test[\"dialog_path\"])\n",
    "image_paths = [os.path.join(data_basedir, location, prefix) \n",
    "                for location, prefix in list(zip(inputs_test[\"image_locations\"],inputs_test[\"image_prefix\"]))]\n",
    "\n",
    "raw_data = json.load(open(dialog_path,'r'))\n",
    "example_raw_data = raw_data['data']['dialogs'][example_index]\n",
    "options_str = example_raw_data['dialog'][question_turn_index]['answer_options']\n",
    "num_turns = len(example_raw_data['dialog'])\n",
    "\n",
    "visualize_predictions(example_raw_data, \n",
    "                      raw_data['data']['questions'], \n",
    "                      raw_data['data']['answers'], \n",
    "                      image_paths,\n",
    "                      sortedScoreAll)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = json.load(open(dialog_path,'r'))\n",
    "image_feats_h5 = h5py.File(params['inputImg'], 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_index = 4\n",
    "question_turn_index = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dialog_example = loaded_data['data']['dialogs'][example_index]\n",
    "visualize_example(dialog_example, loaded_data['data']['questions'], loaded_data['data']['answers'], \n",
    "                  image_paths, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_turn = dialog_example['dialog'][question_turn_index]\n",
    "history = dialog_example['dialog'][:question_turn_index]\n",
    "img_path = get_image_path(dialog_example['image_id'], image_paths)\n",
    "img_feats = torch.FloatTensor(image_feats_h5[\"images_test\"][example_index]).unsqueeze(0)\n",
    "\n",
    "print(\"q turn = \", question_turn)\n",
    "print(\"history = \", history)\n",
    "print(\"img path = \", img_path)\n",
    "print(\"img_feats shape = \", img_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_indices = question_turn['answer_options']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data['data']['questions'][32059]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display loaded image\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "display(Image(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_map = lambda words: np.array([info['word2ind'].get(word, info['word2ind']['UNK']) \n",
    "                                  for word in words], dtype='int64')\n",
    "\n",
    "tokenize = lambda string: ['<START>'] + word_tokenize(string) + ['<END>']\n",
    "\n",
    "# Process image\n",
    "def transform(img):\n",
    "    img = img.astype(\"float\")/255\n",
    "    img = resize(img, (224, 224), mode='constant')\n",
    "    img[:,:,0] -= 0.485\n",
    "    img[:,:,1] -= 0.456\n",
    "    img[:,:,2] -= 0.406\n",
    "    return img.transpose([2,0,1])\n",
    "\n",
    "raw_img = transform(skimage.io.imread(img_path))\n",
    "\n",
    "# Process caption\n",
    "#caption_tokens = tokenize(hist_info['caption'])\n",
    "caption_tokens = tokenize(dialog_example['caption'])\n",
    "caption = ind_map(caption_tokens)\n",
    "\n",
    "# Process history\n",
    "h_question_tokens = []\n",
    "h_questions = []\n",
    "h_answer_tokens = []\n",
    "h_answers = []\n",
    "#for round_idx, item in enumerate(hist_info['dialog']):\n",
    "for round_idx, item in enumerate(history):\n",
    "\n",
    "    #ans_tokens = tokenize(item['answer'])\n",
    "    ans_tokens = tokenize(loaded_data['data']['answers'][item['answer']])\n",
    "    h_answer_tokens.append(ans_tokens)\n",
    "    h_answers.append(ind_map(ans_tokens))\n",
    "    \n",
    "    #ques_tokens = tokenize(item['question'])\n",
    "    ques_tokens = tokenize(loaded_data['data']['questions'][item['question']])\n",
    "    h_question_tokens.append(ques_tokens)\n",
    "    h_questions.append(ind_map(ques_tokens))\n",
    "    \n",
    "# Process question\n",
    "#question_tokens = tokenize(ques_info['question'])\n",
    "question_tokens = tokenize(loaded_data['data']['questions'][question_turn['question']])\n",
    "question = ind_map(question_tokens)\n",
    "\n",
    "# Process options\n",
    "options_tokens = []\n",
    "options = []\n",
    "for opt in options_indices:\n",
    "    opt_tokens = tokenize(loaded_data['data']['answers'][opt])\n",
    "    options_tokens.append(opt_tokens)\n",
    "    options.append(ind_map(opt_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_map(tensor):\n",
    "    if params['useGPU']:\n",
    "        tensor = tensor.cuda()\n",
    "    return Variable(tensor.unsqueeze(0), volatile=True)\n",
    "\n",
    "#img_tensor = var_map(torch.from_numpy(raw_img).float())\n",
    "#img_feats = feat_extract_model(img_tensor)\n",
    "#_norm = torch.norm(img_feats, p=2, dim=1)\n",
    "#img_feats = img_feats.div(_norm.expand_as(img_feats))ffe\n",
    "\n",
    "caption_tensor = var_map(torch.from_numpy(caption))\n",
    "caption_lens = var_map(torch.LongTensor([len(caption)]))\n",
    "\n",
    "question_tensor = var_map(torch.from_numpy(question))\n",
    "question_lens = var_map(torch.LongTensor([len(question)]))\n",
    "\n",
    "hist_ans_tensors = [var_map(torch.from_numpy(ans)) for ans in h_answers]\n",
    "hist_ans_lens = [var_map(torch.LongTensor([len(h_ans)])) for h_ans in h_answer_tokens]\n",
    "hist_ques_tensors = [var_map(torch.from_numpy(ques)) for ques in h_questions]\n",
    "hist_ques_lens = [var_map(torch.LongTensor([len(h_ques)])) for h_ques in h_question_tokens]\n",
    "\n",
    "options_tensors = [var_map(torch.from_numpy(opt)) for opt in options]\n",
    "options_lens = [var_map(torch.LongTensor([len(opt)])) for opt in options_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_ques_tensors[0]\n",
    "hist_ques_lens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_tokens[0]\n",
    "options_lens[0].data[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(options_tensors[0].shape)\n",
    "print(options_tensors[1].shape)\n",
    "print(options_tensors[2].shape)\n",
    "\n",
    "\n",
    "options_tensors_cat = Variable(torch.LongTensor(len(options_tensors), 20+2+1).fill_(0)) #20=max ans len\n",
    "\n",
    "options_tensors_cat[:, 0] = startToken\n",
    "\n",
    "for ansId in range(len(options_tensors)):\n",
    "    length = options_lens[ansId].data[0][0]\n",
    "    if length == 0:\n",
    "        print('Warning: Skipping empty option answer list at (%d)'\\\n",
    "                %ansId)\n",
    "        continue\n",
    "\n",
    "    options_tensors_cat[ansId, 1:length + 1] = options_tensors[ansId][0][:length]\n",
    "    options_tensors_cat[ansId, length + 1] = endToken\n",
    "\n",
    "\n",
    "options_tensors_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for converting tensors to words\n",
    "to_str_pred = lambda w, l: str(\" \".join([info['ind2word'][x] for x in list( filter(\n",
    "        lambda x:x>0,w.data.cpu().numpy()))][:l.data.cpu()[0]]))[8:]\n",
    "to_str_gt = lambda w: str(\" \".join([info['ind2word'][x] for x in filter(\n",
    "        lambda x:x>0,w.data.cpu().numpy())]))[8:-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_tensors_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_str_pred(question_tensor[0], question_lens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_str_gt(question_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tensor[0].shape\n",
    "hist_ques_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_str_pred(hist_ques_tensors[1][0], hist_ques_lens[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_str_gt(hist_ques_tensors[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tensor[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_ques_tensors[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if aBot:\n",
    "    aBot.eval(), aBot.reset()\n",
    "    aBot.observe(\n",
    "        -1, image=img_feats, caption=caption_tensor, captionLens=caption_lens)\n",
    "\n",
    "if qBot:\n",
    "    qBot.eval(), qBot.reset()\n",
    "    qBot.observe(-1, caption=caption_tensor, captionLens=caption_lens)\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "display(Image(img_path))\n",
    "\n",
    "print(\"Caption: \", to_str_gt(caption_tensor[0]))\n",
    "    \n",
    "#numRounds = len(hist_info['dialog'])\n",
    "numRounds = len(history)\n",
    "beamSize = params['beamSize']\n",
    "for round in range(numRounds):\n",
    "    if qBot is None:\n",
    "        aBot.observe(\n",
    "            round,\n",
    "            ques=hist_ques_tensors[round],\n",
    "            quesLens=hist_ques_lens[round])\n",
    "        aBot.observe(\n",
    "            round,\n",
    "            ans=hist_ans_tensors[round],\n",
    "            ansLens=hist_ans_lens[round])\n",
    "        _ = aBot.forward()\n",
    "        answers, ansLens = aBot.forwardDecode(\n",
    "            inference='greedy', beamSize=beamSize)\n",
    "\n",
    "    elif aBot is not None and qBot is not None:\n",
    "        questions, quesLens = qBot.forwardDecode(\n",
    "            beamSize=beamSize, inference='greedy')\n",
    "        qBot.observe(round, ques=questions, quesLens=quesLens)\n",
    "        aBot.observe(round, ques=questions, quesLens=quesLens)\n",
    "        answers, ansLens = aBot.forwardDecode(\n",
    "            beamSize=beamSize, inference='greedy')\n",
    "        aBot.observe(round, ans=answers, ansLens=ansLens)\n",
    "        qBot.observe(round, ans=answers, ansLens=ansLens)\n",
    "        \n",
    "    print(\"Q%d: \"%(round+1), to_str_gt(hist_ques_tensors[round][0]))\n",
    "    print(\"A%d: \"%(round+1), to_str_gt(hist_ans_tensors[round][0]))\n",
    "        \n",
    "# After processing history\n",
    "if qBot is None:\n",
    "    aBot.observe(\n",
    "        numRounds,\n",
    "        ques=question_tensor,\n",
    "        quesLens=question_lens)\n",
    "    answers, ansLens = aBot.forwardDecode(\n",
    "        inference='greedy', beamSize=beamSize)\n",
    "    \n",
    "    # Printing\n",
    "    print(\"Q%d: \"%(numRounds+1), to_str_gt(question_tensor[0]))\n",
    "    print(\"A%d: \"%(numRounds+1), to_str_pred(answers[0], ansLens[0]))\n",
    "    \n",
    "elif aBot is not None and qBot is not None:\n",
    "    questions, quesLens = qBot.forwardDecode(\n",
    "        beamSize=beamSize, inference='greedy')\n",
    "    qBot.observe(numRounds, ques=questions, quesLens=quesLens)\n",
    "    aBot.observe(numRounds, ques=questions, quesLens=quesLens)\n",
    "    answers, ansLens = aBot.forwardDecode(\n",
    "        beamSize=beamSize, inference='greedy')\n",
    "    aBot.observe(numRounds, ans=answers, ansLens=ansLens)\n",
    "    qBot.observe(numRounds, ans=answers, ansLens=ansLens)\n",
    "\n",
    "    # Printing\n",
    "    print(\"Q%d: \"%(numRounds+1), to_str_pred(questions[0], quesLens[0]))\n",
    "    print(\"A%d: \"%(numRounds+1), to_str_pred(answers[0], ansLens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if aBot:\n",
    "    aBot.eval(), aBot.reset()\n",
    "    aBot.observe(\n",
    "        -1, image=img_feats, caption=caption_tensor, captionLens=caption_lens)\n",
    "\n",
    "if qBot:\n",
    "    qBot.eval(), qBot.reset()\n",
    "    qBot.observe(-1, caption=caption_tensor, captionLens=caption_lens)\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "display(Image(img_path))\n",
    "\n",
    "print(\"Caption: \", to_str_gt(caption_tensor[0]))\n",
    "    \n",
    "#numRounds = len(hist_info['dialog'])\n",
    "numRounds = len(history)\n",
    "beamSize = params['beamSize']\n",
    "ranks = []\n",
    "for round in range(numRounds):\n",
    "    if qBot is None:\n",
    "        aBot.observe(\n",
    "            round,\n",
    "            ques=questions[:, round],\n",
    "            quesLens=quesLens[:, round],\n",
    "            ans=answers[:, round],\n",
    "            ansLens=ansLens[:, round])\n",
    "        logProbs = aBot.evalOptions(options[:, round],\n",
    "                                    optionLens[:, round], scoringFunction)\n",
    "        print(\"log probs = \", logProbs)\n",
    "        logProbsCurrent = aBot.forward()\n",
    "        logProbsAll[round].append(\n",
    "            scoringFunction(logProbsCurrent,\n",
    "                            answers[:, round].contiguous()))\n",
    "        batchRanks = rankOptions(options[:, round],\n",
    "                                 correctOptionInds[:, round], logProbs)\n",
    "        ranks.append(batchRanks)\n",
    "            \n",
    "\n",
    "    elif aBot is not None and qBot is not None:\n",
    "        questions, quesLens = qBot.forwardDecode(\n",
    "            beamSize=beamSize, inference='greedy')\n",
    "        qBot.observe(round, ques=questions, quesLens=quesLens)\n",
    "        aBot.observe(round, ques=questions, quesLens=quesLens)\n",
    "        answers, ansLens = aBot.forwardDecode(\n",
    "            beamSize=beamSize, inference='greedy')\n",
    "        aBot.observe(round, ans=answers, ansLens=ansLens)\n",
    "        qBot.observe(round, ans=answers, ansLens=ansLens)\n",
    "        \n",
    "    print(\"Q%d: \"%(round+1), to_str_gt(hist_ques_tensors[round][0]))\n",
    "    print(\"A%d: \"%(round+1), to_str_gt(hist_ans_tensors[round][0]))\n",
    "        \n",
    "# After processing history\n",
    "if qBot is None:\n",
    "    aBot.observe(\n",
    "        numRounds,\n",
    "        ques=question_tensor,\n",
    "        quesLens=question_lens)\n",
    "    answers, ansLens = aBot.forwardDecode(\n",
    "        inference='greedy', beamSize=beamSize)\n",
    "    \n",
    "    # Printing\n",
    "    print(\"Q%d: \"%(numRounds+1), to_str_gt(question_tensor[0]))\n",
    "    print(\"A%d: \"%(numRounds+1), to_str_pred(answers[0], ansLens[0]))\n",
    "    \n",
    "elif aBot is not None and qBot is not None:\n",
    "    questions, quesLens = qBot.forwardDecode(\n",
    "        beamSize=beamSize, inference='greedy')\n",
    "    qBot.observe(numRounds, ques=questions, quesLens=quesLens)\n",
    "    aBot.observe(numRounds, ques=questions, quesLens=quesLens)\n",
    "    answers, ansLens = aBot.forwardDecode(\n",
    "        beamSize=beamSize, inference='greedy')\n",
    "    aBot.observe(numRounds, ans=answers, ansLens=ansLens)\n",
    "    qBot.observe(numRounds, ans=answers, ansLens=ansLens)\n",
    "\n",
    "    # Printing\n",
    "    print(\"Q%d: \"%(numRounds+1), to_str_pred(questions[0], quesLens[0]))\n",
    "    print(\"A%d: \"%(numRounds+1), to_str_pred(answers[0], ansLens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
