{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "from six import iteritems\n",
    "import h5py\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "\n",
    "import options\n",
    "from utils import utilities as utils\n",
    "from dataloader import VisDialDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from eval_utils.rank_answerer import rankABot\n",
    "from eval_utils.rank_questioner import rankQBot\n",
    "from utils import utilities as utils\n",
    "from utils.visualize import VisdomVisualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad image id to 12 digits with zeros out front, and add jpg extension\n",
    "def image_id_to_suffix(image_id):\n",
    "    return str(image_id).zfill(12) + '.jpg'\n",
    "\n",
    "def get_image_path(image_id, images_path):\n",
    "    for path in images_path:\n",
    "        filename = path + image_id_to_suffix(image_id)\n",
    "        if os.path.exists(filename):\n",
    "            #info_json['images'].append({'id':image_id,'file_path':\n",
    "            #                  os.path.join(os.path.basename(os.path.dirname(path)), os.path.basename(filename))})\n",
    "            #second to last dir in path is train2014 or val2014, join with image filename\n",
    "            return filename\n",
    "        \n",
    "    raise ValueError(\"Image id \\\"{}\\\" could not be found in given paths \\\"{}\\\"\"\n",
    "                     .format(image_id, images_path))\n",
    "\n",
    "\n",
    "def visualize_example(dialogue_entry, questions, answers, images_path, verbose=False):\n",
    "    image_id = dialogue_entry['image_id']\n",
    "    image_filename = get_image_path(image_id, images_path)\n",
    "    if image_filename is not None:\n",
    "        display(Image(filename=image_filename))\n",
    "    else:\n",
    "        print(\"Image could not be found.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nDialogue entry: \\n{}\".format(dialogue_entry))\n",
    "        print(\"Image from filename {}\\n\".format(image_filename))\n",
    "        \n",
    "    print(\"Caption = \\\"{}\\\"\\n\".format(dialogue_entry['caption']))\n",
    "    for turn in dialogue_entry['dialog']:\n",
    "        question_id = turn['question']\n",
    "        print(\"Question = \\\"{}\\\"\".format(questions[question_id]))\n",
    "        if 'answer' in turn:\n",
    "            answer_id = turn['answer']\n",
    "            print(\"\\t\\t\\t\\tAnswer = \\\"{}\\\"\\n\".format(answers[answer_id]))\n",
    "        else:\n",
    "            answer_options_ids = turn['answer_options']\n",
    "            print(\"\\t\\t\\t\\tAnswer options = \\n{}\".format([answers[a_id] for a_id in answer_options_ids]))\n",
    "            \n",
    "\n",
    "def visualize_predictions(dialogue_entry, questions, answers, images_path, sortedScoreAll, verbose=False):\n",
    "    \n",
    "    image_id = dialogue_entry['image_id']\n",
    "    #image_filenames = [path + str(dialogue_entry['image_id']).zfill(12) + '.jpg' for path in images_path]\n",
    "    image_filename = get_image_path(image_id, images_path)\n",
    "    display(Image(filename=image_filename))\n",
    "    \n",
    "        \n",
    "    print(\"Caption = \\\"{}\\\"\\n\".format(dialogue_entry['caption']))\n",
    "    for i, turn in enumerate(dialogue_entry['dialog']):\n",
    "        question_id = turn['question']\n",
    "        print(\"\\nQuestion = \\\"{}\\\"\".format(questions[question_id]))\n",
    "        if 'answer' in turn:\n",
    "            answer_id = turn['answer']\n",
    "            print(\"\\t\\t\\t\\tGround Truth Answer = \\\"{}\\\"\\n\".format(answers[answer_id]))\n",
    "        else:\n",
    "            answer_options_ids = turn['answer_options']\n",
    "            print(\"\\t\\t\\t\\tAnswer options = \\n{}\".format([answers[a_id] for a_id in answer_options_ids]))\n",
    "        #print(\"i=\",i)\n",
    "        #print(question_turn_index)\n",
    "        #print(len(dialogue_entry['dialog'])+i)\n",
    "        if i==question_turn_index or i==len(dialogue_entry['dialog'])+question_turn_index:\n",
    "            sortedScores, sortedInds = sortedScoreAll[i]\n",
    "            sortedAnswers = [options_str[i] for i in sortedInds.data[0]]\n",
    "            sortedAnswersStr = [answers[i] for i in sortedAnswers]\n",
    "            print(\"\\t\\t\\t\\tSelected answers = {}\".format(list(zip(sortedAnswersStr, list(sortedScores[0].data)))))\n",
    "            \n",
    "            \n",
    "def rankOptions(options, scores):\n",
    "    '''Rank a batch of examples against a list of options.'''\n",
    "    numOptions = options.size(1)\n",
    "    \n",
    "    # Sort all predicted scores\n",
    "    sortedScore, sortedInds = torch.sort(scores, 1, descending=True)\n",
    "    #print(\"s = \", scores)\n",
    "    #print(\"ss = \", sortedScore)\n",
    "    #print(\"si = \", sortedInds)\n",
    "    \n",
    "    #sortedAnswers = [options_str[i] for i in sortedInds.data[0]]\n",
    "    #sortedAnswersStr = [raw_data['data']['answers'][i] for i in sortedAnswers]\n",
    "    #print(\"sa = \", sortedAnswersStr)\n",
    "    return sortedScore, sortedInds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parse dataset\n",
    "\n",
    "static_params = {\n",
    "    'numRounds':10,\n",
    "    'useGPU': False,\n",
    "    'imgNorm': 0,\n",
    "    \n",
    "    'inputImg': 'data/visdial/data_1.0_img.h5',    \n",
    "    \n",
    "    'inputJson': \"/scr/anarc/motm/data/visdial/data/visdial_params_v2.json\",\n",
    "    'inputQues': '/scr/anarc/motm/data/visdial/data/visdial_data_v2.h5',\n",
    "    'cocoDir': '/scr/anarc/motm/data/visdial/data/visdial_images',\n",
    "    'cocoInfo': '/scr/anarc/motm/data/visdial/data/visdial_images/coco_info.json',\n",
    "}\n",
    "\n",
    "splits = ['val','test']\n",
    "dataset = VisDialDataset(static_params, splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw, unparsed data from json\n",
    "inputs_val = {\n",
    "                \"dialog_path\":\"visdial_1.0_val.json\",\n",
    "                \"image_locations\":[\"visdial_images/VisualDialog_val2018\"],\n",
    "                \"image_prefix\": [\"VisualDialog_val2018_\"]\n",
    "             }\n",
    "\n",
    "inputs_test = {\n",
    "                \"dialog_path\":\"visdial_1.0_test.json\",\n",
    "                \"image_locations\":[\"visdial_images/VisualDialog_test2018\"],\n",
    "                \"image_prefix\": [\"VisualDialog_test2018_\"]\n",
    "               }  \n",
    "data_basedir = \"../data/visdial/data\"\n",
    "\n",
    "\n",
    "val_dialog_path = os.path.join(data_basedir, inputs_val[\"dialog_path\"])\n",
    "val_image_paths = [os.path.join(data_basedir, location, prefix) \n",
    "                for location, prefix in list(zip(inputs_val[\"image_locations\"],inputs_val[\"image_prefix\"]))]\n",
    "val_raw_data = json.load(open(val_dialog_path,'r'))\n",
    "\n",
    "\n",
    "test_dialog_path = os.path.join(data_basedir, inputs_test[\"dialog_path\"])\n",
    "test_image_paths = [os.path.join(data_basedir, location, prefix) \n",
    "                for location, prefix in list(zip(inputs_test[\"image_locations\"],inputs_test[\"image_prefix\"]))]\n",
    "test_raw_data = json.load(open(test_dialog_path,'r'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_category_mapping = json.load(open('data/qa_category_mapping.json','r'))\n",
    "\n",
    "cat = \"color\"\n",
    "split = \"val\"\n",
    "print(\"examples for category \\\"{}\\\":\".format(cat))\n",
    "print(qa_category_mapping[split][cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_category_mapping.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurable Parameters and Example Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \n",
    "    # A-Bot checkpoint\n",
    "    #'startFrom': \"./checkpoints/all_duplicate_duplicate_duplicate/abot_ep_22.vd\",\n",
    "    'startFrom': \"./checkpoints/color/abot_ep_37.vd\",\n",
    "    \n",
    "    # Q-Bot checkpoint should given if interactive dialog is required\n",
    "    # 'qstartFrom': \"./checkpoints/qbot_sl.vd\",\n",
    "    \n",
    "    'beamSize': 5,\n",
    "    'imgFeatureSize':16384,\n",
    "    \n",
    "}\n",
    "\n",
    "for key, value in iteritems(static_params):\n",
    "    params[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_split = 'val'\n",
    "example_index = 4\n",
    "question_turn_index = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.split = example_split\n",
    "\n",
    "example_parsed = dataset[example_index]\n",
    "example_batched = dataset.collate_fn([example_parsed])\n",
    "print(\"Parsed example from dataset = \\n{}\".format(example_parsed))\n",
    "print(\"Batched parsed example from dataset = \\n{}\".format(example_batched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = val_raw_data if example_split=='val' else test_raw_data\n",
    "image_paths = val_image_paths if example_split=='val' else test_image_paths\n",
    "\n",
    "\n",
    "\n",
    "example_raw_data = raw_data['data']['dialogs'][example_index]\n",
    "options_str = example_raw_data['dialog'][question_turn_index]['answer_options']\n",
    "num_turns = len(example_raw_data['dialog'])\n",
    "\n",
    "visualize_example(example_raw_data, \n",
    "                  raw_data['data']['questions'], \n",
    "                  raw_data['data']['answers'], \n",
    "                  image_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and load and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# RNG seed\n",
    "manualSeed = 1597\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "if params['useGPU']:\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "print('Loading json file: ' + params['inputJson'])\n",
    "with open(params['inputJson'], 'r') as fileId:\n",
    "    info = json.load(fileId)\n",
    "\n",
    "wordCount = len(info['word2ind'])\n",
    "# Add <START> and <END> to vocabulary\n",
    "info['word2ind']['<START>'] = wordCount + 1\n",
    "info['word2ind']['<END>'] = wordCount + 2\n",
    "startToken = info['word2ind']['<START>']\n",
    "endToken = info['word2ind']['<END>']\n",
    "# Padding token is at index 0\n",
    "vocabSize = wordCount + 3\n",
    "print('Vocab size with <START>, <END>: %d' % vocabSize)\n",
    "\n",
    "# Construct the reverse map\n",
    "info['ind2word'] = {\n",
    "    int(ind): word\n",
    "    for word, ind in info['word2ind'].items()\n",
    "}\n",
    "    \n",
    "def loadModel(params, agent='abot'):\n",
    "    # should be everything used in encoderParam, decoderParam below\n",
    "    encoderOptions = [\n",
    "        'encoder', 'vocabSize', 'embedSize', 'rnnHiddenSize', 'numLayers',\n",
    "        'useHistory', 'useIm', 'imgEmbedSize', 'imgFeatureSize', 'numRounds',\n",
    "        'dropout'\n",
    "    ]\n",
    "    decoderOptions = [\n",
    "        'decoder', 'vocabSize', 'embedSize', 'rnnHiddenSize', 'numLayers',\n",
    "        'dropout'\n",
    "    ]\n",
    "    modelOptions = encoderOptions + decoderOptions\n",
    "\n",
    "    mdict = None\n",
    "    gpuFlag = params['useGPU']\n",
    "    startArg = 'startFrom' if agent == 'abot' else 'qstartFrom'\n",
    "    assert params[startArg], \"Need checkpoint for {}\".format(agent)\n",
    "\n",
    "    if params[startArg]:\n",
    "        print('Loading model (weights and config) from {}'.format(\n",
    "            params[startArg]))\n",
    "\n",
    "        if gpuFlag:\n",
    "            mdict = torch.load(params[startArg])\n",
    "        else:\n",
    "            mdict = torch.load(params[startArg],\n",
    "                map_location=lambda storage, location: storage)\n",
    "\n",
    "        # Model options is a union of standard model options defined\n",
    "        # above and parameters loaded from checkpoint\n",
    "        modelOptions = list(set(modelOptions).union(set(mdict['params'])))\n",
    "        for opt in modelOptions:\n",
    "            if opt not in params:\n",
    "                params[opt] = mdict['params'][opt]\n",
    "\n",
    "            elif params[opt] != mdict['params'][opt]:\n",
    "                # Parameters are not overwritten from checkpoint\n",
    "                pass\n",
    "\n",
    "    # Initialize model class\n",
    "    encoderParam = {k: params[k] for k in encoderOptions}\n",
    "    decoderParam = {k: params[k] for k in decoderOptions}\n",
    "\n",
    "    encoderParam['startToken'] = encoderParam['vocabSize'] - 2\n",
    "    encoderParam['endToken'] = encoderParam['vocabSize'] - 1\n",
    "    decoderParam['startToken'] = decoderParam['vocabSize'] - 2\n",
    "    decoderParam['endToken'] = decoderParam['vocabSize'] - 1\n",
    "\n",
    "    if agent == 'abot':\n",
    "        encoderParam['type'] = params['encoder']\n",
    "        decoderParam['type'] = params['decoder']\n",
    "        encoderParam['isAnswerer'] = True\n",
    "        from visdial.models.answerer import Answerer\n",
    "        model = Answerer(encoderParam, decoderParam)\n",
    "        print(\"e param = \", encoderParam)\n",
    "        print(\"e = \", model.encoder)\n",
    "\n",
    "    elif agent == 'qbot':\n",
    "        encoderParam['type'] = params['qencoder']\n",
    "        decoderParam['type'] = params['qdecoder']\n",
    "        encoderParam['isAnswerer'] = False\n",
    "        encoderParam['useIm'] = False\n",
    "        from visdial.models.questioner import Questioner\n",
    "        model = Questioner(\n",
    "            encoderParam,\n",
    "            decoderParam,\n",
    "            imgFeatureSize=encoderParam['imgFeatureSize'])\n",
    "\n",
    "    if params['useGPU']:\n",
    "        model.cuda()\n",
    "\n",
    "    if mdict:\n",
    "        model.load_state_dict(mdict['model'])\n",
    "        \n",
    "    print(\"Loaded agent {}\".format(agent))\n",
    "    return model\n",
    "\n",
    "aBot = None\n",
    "qBot = None\n",
    "\n",
    "# load aBot\n",
    "if params['startFrom']:\n",
    "    aBot = loadModel(params, 'abot')\n",
    "    assert aBot.encoder.vocabSize == vocabSize, \"Vocab size mismatch!\"\n",
    "    aBot.eval()\n",
    "\n",
    "# load qBot\n",
    "if params['qstartFrom']:\n",
    "    qBot = loadModel(params, 'qbot')\n",
    "    assert qBot.encoder.vocabSize == vocabSize, \"Vocab size mismatch!\"\n",
    "    qBot.eval()\n",
    "\n",
    "# load pre-trained VGG 19\n",
    "print(\"Loading image feature extraction model\")\n",
    "feat_extract_model = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "feat_extract_model.classifier = nn.Sequential(*list(feat_extract_model.classifier.children())[:-3])\n",
    "# print(feat_extract_model)\n",
    "feat_extract_model.eval()\n",
    "\n",
    "if params['useGPU']:\n",
    "    feat_extract_model.cuda()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Variable(example_batched['img_feat'], volatile=True)\n",
    "caption = Variable(example_batched['cap'], volatile=True)\n",
    "captionLens = Variable(example_batched['cap_len'], volatile=True)\n",
    "questions = Variable(example_batched['ques'], volatile=True)\n",
    "quesLens = Variable(example_batched['ques_len'], volatile=True)\n",
    "answers = Variable(example_batched['ans'], volatile=True)\n",
    "ansLens = Variable(example_batched['ans_len'], volatile=True)\n",
    "options = Variable(example_batched['opt'], volatile=True)\n",
    "optionLens = Variable(example_batched['opt_len'], volatile=True)\n",
    "#correctOptionInds = Variable(example_batched['ans_id'], volatile=True)\n",
    "\n",
    "numRounds = dataset.numRounds\n",
    "sortedScoreAll = []\n",
    "logProbsAll = [[] for _ in range(numRounds)]\n",
    "scoringFunction=utils.maskedNll\n",
    "\n",
    "aBot.reset()\n",
    "aBot.observe(-1, image=image, caption=caption, captionLens=captionLens)\n",
    "for round in range(numRounds):\n",
    "    print(\"Round = \", round)\n",
    "    if quesLens[0][round].data[0] == 1:\n",
    "        print(\"skipping round\")\n",
    "        continue\n",
    "    \n",
    "    aBot.observe(\n",
    "        round,\n",
    "        ques=questions[:, round],\n",
    "        quesLens=quesLens[:, round],\n",
    "        ans=answers[:, round],\n",
    "        ansLens=ansLens[:, round])\n",
    "    #print(\"opt = \", options[:,round])\n",
    "    logProbs = aBot.evalOptions(options[:, round],\n",
    "                                optionLens[:, round], scoringFunction)\n",
    "    #print(\"lp = \", logProbs)\n",
    "    logProbsCurrent = aBot.forward()\n",
    "    #print(\"lpc = \", logProbsCurrent)\n",
    "    logProbsAll[round].append(\n",
    "        scoringFunction(logProbsCurrent,\n",
    "                        answers[:, round].contiguous()))\n",
    "    #print(\"lpa = \", logProbsAll)\n",
    "    sortedScore, sortedInds = rankOptions(options[:,round], logProbs)\n",
    "    sortedScoreAll.append((sortedScore,sortedInds))\n",
    "    #batchRanks = rankOptions(options[:, round],\n",
    "    #                         correctOptionInds[:, round], logProbs)\n",
    "    #ranks.append(batchRanks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(example_raw_data, \n",
    "                      raw_data['data']['questions'], \n",
    "                      raw_data['data']['answers'], \n",
    "                      image_paths,\n",
    "                      sortedScoreAll)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
